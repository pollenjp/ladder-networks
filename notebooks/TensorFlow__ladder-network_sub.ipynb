{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow__ladder-network\n",
    "\n",
    "- 論文\n",
    "  - [Semi-Supervised Learning with Ladder Networks - Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, Tapani Raiko]](https://arxiv.org/abs/1507.02672)\n",
    "\n",
    "- code\n",
    "  - [rinuboney/ladder](https://github.com/rinuboney/ladder)\n",
    "  - [tarvaina/tensorflow-ladder](https://github.com/tarvaina/tensorflow-ladder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Config\" data-toc-modified-id=\"Config-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Config</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#TensorFlow\" data-toc-modified-id=\"TensorFlow-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>TensorFlow</a></span></li><li><span><a href=\"#Path\" data-toc-modified-id=\"Path-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Path</a></span></li><li><span><a href=\"#Import-2\" data-toc-modified-id=\"Import-2-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Import 2</a></span></li><li><span><a href=\"#Class\" data-toc-modified-id=\"Class-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Class</a></span><ul class=\"toc-item\"><li><span><a href=\"#batch_normalization\" data-toc-modified-id=\"batch_normalization-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>batch_normalization</a></span></li><li><span><a href=\"#ladder_network\" data-toc-modified-id=\"ladder_network-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>ladder_network</a></span></li></ul></li></ul></li><li><span><a href=\"#Class\" data-toc-modified-id=\"Class-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Class</a></span><ul class=\"toc-item\"><li><span><a href=\"#class-Session\" data-toc-modified-id=\"class-Session-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>class Session</a></span></li><li><span><a href=\"#class-Graph\" data-toc-modified-id=\"class-Graph-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>class Graph</a></span></li><li><span><a href=\"#class-_Placeholders\" data-toc-modified-id=\"class-_Placeholders-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>class _Placeholders</a></span></li><li><span><a href=\"#class-_ForwardPass\" data-toc-modified-id=\"class-_ForwardPass-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>class _ForwardPass</a></span></li><li><span><a href=\"#class-_InputLayerWrapper\" data-toc-modified-id=\"class-_InputLayerWrapper-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>class _InputLayerWrapper</a></span></li><li><span><a href=\"#class-_EncoderLayer\" data-toc-modified-id=\"class-_EncoderLayer-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>class _EncoderLayer</a></span></li><li><span><a href=\"#class-_DecoderLayer\" data-toc-modified-id=\"class-_DecoderLayer-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>class _DecoderLayer</a></span></li></ul></li><li><span><a href=\"#input_data\" data-toc-modified-id=\"input_data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>input_data</a></span></li><li><span><a href=\"#ladder_on_mnist\" data-toc-modified-id=\"ladder_on_mnist-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ladder_on_mnist</a></span></li><li><span><a href=\"#End\" data-toc-modified-id=\"End-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>End</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:43.935435Z",
     "start_time": "2018-09-14T02:55:43.518083Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules to reload:\n",
      "all-except-skipped\n",
      "\n",
      "Modules to skip:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy ver: 1.15.0\n",
      "scikit-learn ver: 0.19.2\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "# If you want to reload manually, add a below line head.\n",
    "%aimport\n",
    "# ref: https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "\n",
    "import os,sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "seed = None\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "print(\"numpy ver: {}\".format(np.__version__))\n",
    "print(\"scikit-learn ver: {}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.278250Z",
     "start_time": "2018-09-14T02:55:43.936995Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow ver: 1.9.0\n",
      "tf.executing_eagerly(): False\n",
      "\n",
      "________________________________________\n",
      "Visible GPUs from TensorFlow\n",
      "________________________________________\n",
      "\n",
      "(CPU:0)\n",
      "(GPU:0: pci_bus_id: 0000:01:00.0)\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "#____________________________________________________________________________________________________\n",
    "#  TensorFlow and Keras GPU configures\n",
    "##________________________________________________________________________________\n",
    "##  OPTIONAL : set a GPU viewed by TensorFlow\n",
    "###____________________________________________________________\n",
    "###  - https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "##________________________________________________________________________________\n",
    "\n",
    "\n",
    "##________________________________________________________________________________\n",
    "##  TensorFlow\n",
    "###____________________________________________________________\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow ver: {}\".format(tf.__version__))\n",
    "###  eager mode\n",
    "#tf.enable_eager_execution()\n",
    "print(\"tf.executing_eagerly(): {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "# You can double check that you have the correct devices visible to TF\n",
    "#   - https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter\n",
    "from tensorflow.python.client import device_lib\n",
    "print(\"\"\"\n",
    "________________________________________\n",
    "Visible GPUs from TensorFlow\n",
    "________________________________________\"\"\")\n",
    "for _device in device_lib.list_local_devices():\n",
    "    match = re.search(pattern=r'name: \"/device:(?P<name>[A-Z]{3}):(?P<device_num>\\d{1})*',\n",
    "                      string=str(_device))\n",
    "    if match is None:\n",
    "        print(\"Not Match\")\n",
    "        continue\n",
    "    if match.group(\"name\") == \"CPU\":\n",
    "        name, device_num = match.group(\"name\", \"device_num\")\n",
    "        print()\n",
    "        print(\"({}:{})\".format(name, device_num))\n",
    "        continue\n",
    "    name, device_num = match.group(\"name\", \"device_num\")\n",
    "    match = re.search(pattern=r'.*pci bus id: (?P<pci_bus_id>\\d{4}:\\d{2}:\\d{2}.\\d{1}).*',\n",
    "                      string=str(_device))\n",
    "    if match is None:\n",
    "        print(\"No GPUs\")\n",
    "        continue\n",
    "    print(\"({}:{}: pci_bus_id: {})\".format(name, device_num, match.group(\"pci_bus_id\")))\n",
    "print(\"________________________________________\")\n",
    "\n",
    "###____________________________________________________________\n",
    "###  sessioin\n",
    "global _SESSION\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#_SESSION = tf.Session(config=config)\n",
    "###____________________________________________________________\n",
    "##________________________________________________________________________________\n",
    "#____________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.294219Z",
     "start_time": "2018-09-14T02:55:45.279491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pollenjp/workdir/git/article_script/20180914__semi-supervised-deeplearning-ladder-networks__in_kabuku\n"
     ]
    }
   ],
   "source": [
    "HOME = Path(os.getcwd()).parent\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.671095Z",
     "start_time": "2018-09-14T02:55:45.295428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True :  /home/pollenjp/workdir/git/article_script/20180914__semi-supervised-deeplearning-ladder-networks__in_kabuku/data\n",
      "True :  /home/pollenjp/workdir/git/article_script/20180914__semi-supervised-deeplearning-ladder-networks__in_kabuku/data/raw\n",
      "True :  /home/pollenjp/workdir/git/article_script/20180914__semi-supervised-deeplearning-ladder-networks__in_kabuku/data/plot_images\n",
      "True :  /home/pollenjp/workdir/git/article_script/20180914__semi-supervised-deeplearning-ladder-networks__in_kabuku/src\n"
     ]
    }
   ],
   "source": [
    "path_list = []\n",
    "data_Path = HOME / \"data\"\n",
    "path_list.append(data_Path)\n",
    "raw_Path = data_Path / \"raw\"\n",
    "path_list.append(raw_Path)\n",
    "plot_images_Path = data_Path / \"plot_images\"\n",
    "path_list.append(plot_images_Path)\n",
    "src_Path = HOME / \"src\"\n",
    "path_list.append(src_Path)\n",
    "\n",
    "for _Path in path_list:\n",
    "    _path = str(_Path)\n",
    "    if not os.path.exists(_path):\n",
    "        os.makedirs(name=_path)\n",
    "        print(\"make a directory: \\n\\t\", _path)\n",
    "    else:\n",
    "        print(os.path.exists(_path), \": \", _path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.747168Z",
     "start_time": "2018-09-14T02:55:45.672555Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.python import control_flow_ops\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.765444Z",
     "start_time": "2018-09-14T02:55:45.748665Z"
    }
   },
   "outputs": [],
   "source": [
    "## Adapted from http://stackoverflow.com/a/34634291/64979\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import control_flow_ops\n",
    "\n",
    "def batch_norm(inputs, is_training_phase):\n",
    "  \"\"\"\n",
    "  Batch normalization for fully connected layers.\n",
    "  Args:\n",
    "    inputs:            2D Tensor, batch size * layer width\n",
    "    is_training_phase: boolean tf.Variable, true indicates training phase\n",
    "  Return:\n",
    "    normed:            batch-normalized Tensor\n",
    "  \"\"\"\n",
    "  with tf.name_scope('batch_norm') as scope:\n",
    "    depth = inputs.get_shape()[-1].value\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs, [0], name = 'moments')\n",
    "    batch_std = tf.sqrt(batch_var)\n",
    "    ema = tf.train.ExponentialMovingAverage(decay = 0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "\n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "    mean, var = control_flow_ops.cond(is_training_phase,\n",
    "                                      mean_var_with_update,\n",
    "                                      lambda: (ema_mean, ema_var))\n",
    "\n",
    "    normed = (inputs - batch_mean) / batch_std\n",
    "\n",
    "    return normed, batch_mean, batch_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ladder_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.811896Z",
     "start_time": "2018-09-14T02:55:45.766852Z"
    }
   },
   "outputs": [],
   "source": [
    "### class Session\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, graph):\n",
    "        self.session = tf.Session()\n",
    "        self.graph = graph\n",
    "        self.writer = tf.train.SummaryWriter(logdir = strftime(\"logs/%Y-%m-%d_%H-%M-%S\"),\n",
    "                                             graph = self.session.graph)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.session.close()\n",
    "\n",
    "    def train_supervised_batch(self, inputs, labels, step_number):\n",
    "        return self._run(self.graph.supervised_train_step,\n",
    "                         summary_action = self.graph.supervised_summaries,\n",
    "                         step_number = step_number,\n",
    "                         inputs = inputs,\n",
    "                         labels = labels,\n",
    "                         is_training_phase = True)\n",
    "\n",
    "    def train_unsupervised_batch(self, inputs, step_number):\n",
    "        return self._run(self.graph.unsupervised_train_step,\n",
    "                         summary_action = self.graph.unsupervised_summaries,\n",
    "                         step_number = step_number,\n",
    "                         inputs = inputs,\n",
    "                         is_training_phase = True)\n",
    "\n",
    "    def test(self, inputs, labels, step_number):\n",
    "        result = self._run(self.graph.accuracy_measure,\n",
    "                           summary_action = self.graph.test_summaries,\n",
    "                           step_number = step_number,\n",
    "                           inputs = inputs,\n",
    "                           labels = labels,\n",
    "                           is_training_phase = False)\n",
    "        self.writer.flush()\n",
    "        return result\n",
    "\n",
    "    def save(self):\n",
    "        return self.graph.saver.save(self.session, \"checkpoints\")\n",
    "\n",
    "    def _run(self, action, summary_action, step_number, inputs, labels = None, is_training_phase = True):\n",
    "        variable_placements = self.graph.placeholders.placements(inputs, labels, is_training_phase)\n",
    "        action_result, summary = self.session.run([action, summary_action], variable_placements)\n",
    "        self.writer.add_summary(summary, step_number)\n",
    "        return action_result\n",
    "\n",
    "### class Graph\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self,\n",
    "        learning_rate,\n",
    "        noise_level,\n",
    "        input_layer_size,\n",
    "        class_count,\n",
    "        encoder_layer_definitions,\n",
    "        denoising_cost_multipliers):\n",
    "        assert class_count == encoder_layer_definitions[-1][0]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.denoising_cost_multipliers = denoising_cost_multipliers\n",
    "        self.placeholders = _Placeholders(input_layer_size, class_count)\n",
    "        self.output = _ForwardPass(self.placeholders,\n",
    "                                   noise_level=noise_level,\n",
    "                                   encoder_layer_definitions=encoder_layer_definitions)\n",
    "        self.accuracy_measure = self._accuracy_measure(\n",
    "            self.placeholders, self.output)\n",
    "        self.supervised_train_step = self._supervised_train_step(\n",
    "            self.placeholders, self.output)\n",
    "        self.unsupervised_train_step = self._unsupervised_train_step(\n",
    "            self.placeholders, self.output)\n",
    "\n",
    "        self.unsupervised_summaries = tf.merge_all_summaries(\"unsupervised\")\n",
    "        self.supervised_summaries = tf.merge_all_summaries(\"supervised\")\n",
    "        self.test_summaries = tf.merge_all_summaries(\"test\")\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def _accuracy_measure(self, placeholders, output):\n",
    "        with tf.name_scope(\"accuracy_measure\") as scope:\n",
    "            actual_labels = tf.argmax(output.clean_label_probabilities, 1)\n",
    "            expected_labels = tf.argmax(placeholders.labels, 1)\n",
    "            correct_prediction = tf.equal(actual_labels, expected_labels)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            tf.histogram_summary(\"class distribution\", actual_labels, [\"test\"])\n",
    "            tf.scalar_summary(\"test accuracy\", accuracy, [\"test\"])\n",
    "            return accuracy\n",
    "\n",
    "    def _supervised_train_step(self, placeholders, output):\n",
    "        with tf.name_scope(\"supervised_training\") as scope:\n",
    "            total_cost = self._total_cost(placeholders, output)\n",
    "            return self._optimizer(self.learning_rate, total_cost, [\"supervised\"])\n",
    "\n",
    "    def _unsupervised_train_step(self, placeholders, output):\n",
    "        with tf.name_scope(\"unsupervised_training\") as scope:\n",
    "            summary_tags = [\"unsupervised\"]\n",
    "            total_denoising_cost, layer_denoising_costs = self._total_denoising_cost(placeholders, output)\n",
    "            tf.scalar_summary(\"total denoising cost\", total_denoising_cost, summary_tags)\n",
    "            for index, layer_cost in enumerate(layer_denoising_costs):\n",
    "                tf.scalar_summary(\"layer %i denoising cost\" % index, layer_cost, summary_tags)\n",
    "            return self._optimizer(self.learning_rate, total_denoising_cost, summary_tags)\n",
    "\n",
    "    def _optimizer(self, learning_rate, cost_function, summary_tags):\n",
    "        with tf.name_scope(\"optimizer\") as scope:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients_and_vars = optimizer.compute_gradients(cost_function)\n",
    "            for (gradient, var) in gradients_and_vars:\n",
    "                if gradient is not None:\n",
    "                    tf.histogram_summary(\"gradient for %s\" % var.name, gradient, summary_tags)\n",
    "            return optimizer.apply_gradients(gradients_and_vars)\n",
    "\n",
    "    def _total_cost(self, placeholders, output):\n",
    "        with tf.name_scope(\"total_cost\") as scope:\n",
    "            cross_entropy = self._cross_entropy(placeholders, output)\n",
    "            total_denoising_cost, layer_denoising_costs = self._total_denoising_cost(placeholders, output)\n",
    "            total_cost = cross_entropy + total_denoising_cost\n",
    "\n",
    "            self._log_all_costs(total_cost, cross_entropy, total_denoising_cost, layer_denoising_costs, [\"supervised\"])\n",
    "\n",
    "        return total_cost\n",
    "\n",
    "    def _log_all_costs(self,\n",
    "                       total_cost = None, cross_entropy = None,\n",
    "                       total_denoising_cost = None, layer_denoising_costs = None,\n",
    "                       summary_tags = tf.GraphKeys.SUMMARIES):\n",
    "        tf.scalar_summary(\"total cost\", total_cost, summary_tags)\n",
    "\n",
    "        tf.scalar_summary(\"cross entropy\", cross_entropy, summary_tags)\n",
    "        tf.scalar_summary(\"cross entropy %\", 100 * cross_entropy / total_cost, summary_tags)\n",
    "\n",
    "        tf.scalar_summary(\"total denoising cost\", total_denoising_cost, summary_tags)\n",
    "        tf.scalar_summary(\"total denoising cost %\", 100 * total_denoising_cost / total_cost, summary_tags)\n",
    "\n",
    "        for index, layer_cost in enumerate(layer_denoising_costs):\n",
    "            tf.scalar_summary(\"layer %i denoising cost\" % index, layer_cost, summary_tags)\n",
    "            tf.scalar_summary(\"layer %i denoising cost %%\" % index, 100 * layer_cost / total_cost, summary_tags)\n",
    "\n",
    "\n",
    "    def _cross_entropy(self, placeholders, output):\n",
    "        with tf.name_scope(\"cross_entropy_cost\") as scope:\n",
    "            cross_entropy = -tf.reduce_mean(placeholders.labels * tf.log(output.corrupted_label_probabilities))\n",
    "        return cross_entropy\n",
    "\n",
    "    def _total_denoising_cost(self, placeholders, output):\n",
    "        with tf.name_scope(\"denoising_cost\") as scope:\n",
    "            layer_costs = [self._layer_denoising_cost(*params)\n",
    "                           for params in zip(output.clean_encoder_outputs,\n",
    "                                             reversed(output.decoder_outputs),\n",
    "                                             self.denoising_cost_multipliers)]\n",
    "            total_denoising_cost = sum(layer_costs)\n",
    "            return total_denoising_cost, layer_costs\n",
    "\n",
    "    def _layer_denoising_cost(self, encoder, decoder, cost_multiplier):\n",
    "        return cost_multiplier * self._mean_squared_error(encoder.pre_activation, decoder.post_2nd_normalization)\n",
    "\n",
    "    def _mean_squared_error(self, expected, actual):\n",
    "        return tf.reduce_mean(tf.pow(expected - actual, 2))\n",
    "\n",
    "### class _Placeholders\n",
    "\n",
    "class _Placeholders:\n",
    "    def __init__(self, input_layer_size, class_count):\n",
    "        with tf.name_scope(\"placeholders\") as scope:\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, input_layer_size], name = 'inputs')\n",
    "            self.labels = tf.placeholder(tf.float32, [None, class_count], name = 'labels')\n",
    "            self.is_training_phase = tf.placeholder(tf.bool, name = 'is_training_phase')\n",
    "\n",
    "    def placements(self, inputs, labels = None, is_training_phase = True):\n",
    "        if labels is None:\n",
    "            labels = numpy.zeros([inputs.shape[0], _layer_size(self.labels)])\n",
    "        return {\n",
    "            self.inputs: inputs,\n",
    "            self.labels: labels,\n",
    "            self.is_training_phase: is_training_phase\n",
    "        }\n",
    "\n",
    "\n",
    "### class _ForwardPass\n",
    "\n",
    "class _ForwardPass:\n",
    "    def __init__(self, placeholders, encoder_layer_definitions, noise_level):\n",
    "        with tf.name_scope(\"clean_encoder\") as scope:\n",
    "            clean_encoder_outputs = self._encoder_layers(input_layer = placeholders.inputs,\n",
    "                                                         other_layer_definitions = encoder_layer_definitions,\n",
    "                                                         is_training_phase = placeholders.is_training_phase)\n",
    "\n",
    "        with tf.name_scope(\"corrupted_encoder\") as scope:\n",
    "            corrupted_encoder_outputs = self._encoder_layers(input_layer = placeholders.inputs,\n",
    "                                                             other_layer_definitions = encoder_layer_definitions,\n",
    "                                                             is_training_phase = placeholders.is_training_phase,\n",
    "                                                             noise_level = noise_level,\n",
    "                                                             reuse_variables = clean_encoder_outputs[1:])\n",
    "\n",
    "        with tf.name_scope(\"decoder\") as scope:\n",
    "            decoder_outputs = self._decoder_layers(clean_encoder_layers = clean_encoder_outputs,\n",
    "                                                   corrupted_encoder_layers = corrupted_encoder_outputs,\n",
    "                                                   is_training_phase = placeholders.is_training_phase)\n",
    "\n",
    "        self.clean_label_probabilities = clean_encoder_outputs[-1].post_activation\n",
    "        self.corrupted_label_probabilities = corrupted_encoder_outputs[-1].post_activation\n",
    "        self.autoencoded_inputs = decoder_outputs[-1]\n",
    "        self.clean_encoder_outputs = clean_encoder_outputs\n",
    "        self.corrupted_encoder_outputs = corrupted_encoder_outputs\n",
    "        self.decoder_outputs = decoder_outputs\n",
    "\n",
    "    def _encoder_layers(self,\n",
    "                        input_layer, other_layer_definitions,\n",
    "                        noise_level = None, is_training_phase = True, reuse_variables = None):\n",
    "        first_encoder_layer = _InputLayerWrapper(input_layer)\n",
    "        if reuse_variables is None:\n",
    "            reuse_variables = [None for layer in other_layer_definitions]\n",
    "        layer_accumulator = [first_encoder_layer]\n",
    "        for ((layer_size, non_linearity), reuse_layer) in zip(other_layer_definitions, reuse_variables):\n",
    "            layer_output = _EncoderLayer(inputs = layer_accumulator[-1].post_activation,\n",
    "                                         output_size = layer_size,\n",
    "                                         non_linearity = non_linearity,\n",
    "                                         noise_level = noise_level,\n",
    "                                         is_training_phase = is_training_phase,\n",
    "                                         reuse_variables = reuse_layer)\n",
    "            layer_accumulator.append(layer_output)\n",
    "        return layer_accumulator\n",
    "\n",
    "    def _decoder_layers(self, clean_encoder_layers, corrupted_encoder_layers,is_training_phase):\n",
    "        # FIXME: Actually the first decoder layer should get the correct label from above\n",
    "        encoder_layers = reversed(zip(clean_encoder_layers, corrupted_encoder_layers))\n",
    "        layer_accumulator = [None]\n",
    "        for clean_layer, corrupted_layer in encoder_layers:\n",
    "            layer = _DecoderLayer(clean_encoder_layer = clean_layer,\n",
    "                                  corrupted_encoder_layer = corrupted_layer,\n",
    "                                  previous_decoder_layer = layer_accumulator[-1],\n",
    "                                  is_training_phase = is_training_phase)\n",
    "            layer_accumulator.append(layer)\n",
    "        return layer_accumulator[1:]\n",
    "\n",
    "### class _InputLayerWrapper\n",
    "\n",
    "class _InputLayerWrapper:\n",
    "    def __init__(self, input_layer):\n",
    "        self.pre_activation = input_layer\n",
    "        self.post_activation = input_layer\n",
    "        self.batch_mean = tf.zeros_like(input_layer)\n",
    "        self.batch_std = tf.ones_like(input_layer)\n",
    "\n",
    "### class _EncoderLayer\n",
    "\n",
    "class _EncoderLayer:\n",
    "    def __init__(self, inputs, output_size, non_linearity,\n",
    "                 noise_level, is_training_phase, reuse_variables = None):\n",
    "        with tf.name_scope(\"encoder_layer\") as scope:\n",
    "            self._create_or_reuse_variables(reuse_variables, _layer_size(inputs), output_size)\n",
    "            self.pre_normalization = tf.matmul(inputs, self.weights)\n",
    "            pre_noise, self.batch_mean, self.batch_std = batch_norm(self.pre_normalization,\n",
    "                                                                    is_training_phase = is_training_phase)\n",
    "            self.pre_activation = self._add_noise(pre_noise, noise_level)\n",
    "            beta_gamma = self.gamma * (self.pre_activation + self.beta)\n",
    "            self.post_activation = non_linearity(beta_gamma)\n",
    "\n",
    "    def _create_or_reuse_variables(self, variables, input_size, output_size):\n",
    "        if variables is None:\n",
    "            self.weights = _weight_variable([input_size, output_size], name = 'W')\n",
    "            self.beta = tf.Variable(tf.constant(0.0, shape = [output_size]), name = 'beta')\n",
    "            self.gamma = tf.Variable(tf.constant(1.0, shape = [output_size]), name = 'gamma')\n",
    "        else:\n",
    "            self.weights = variables.weights\n",
    "            self.beta = variables.beta\n",
    "            self.gamma = variables.gamma\n",
    "\n",
    "    def _add_noise(self, tensor, noise_level):\n",
    "        if noise_level is None:\n",
    "            return tensor\n",
    "        else:\n",
    "            return tensor + tf.random_normal([_layer_size(tensor)], mean = 0.0, stddev = noise_level)\n",
    "\n",
    "### class _DecoderLayer\n",
    "\n",
    "class _DecoderLayer:\n",
    "    def __init__(self, clean_encoder_layer, corrupted_encoder_layer,\n",
    "                 previous_decoder_layer = None, is_training_phase = True):\n",
    "        with tf.name_scope(\"decoder_layer\") as scope:\n",
    "            is_first_decoder_layer = previous_decoder_layer is None\n",
    "            if is_first_decoder_layer:\n",
    "                pre_1st_normalization = corrupted_encoder_layer.post_activation\n",
    "            else:\n",
    "                input_size = _layer_size(previous_decoder_layer.post_denoising)\n",
    "                output_size = _layer_size(clean_encoder_layer.post_activation)\n",
    "                weights = _weight_variable([input_size, output_size], name = 'V')\n",
    "                pre_1st_normalization = tf.matmul(previous_decoder_layer.post_denoising, weights)\n",
    "\n",
    "        pre_denoising, _, _ = batch_norm(pre_1st_normalization, is_training_phase = is_training_phase)\n",
    "        post_denoising = self._denoise(corrupted_encoder_layer.pre_activation, pre_denoising)\n",
    "        post_2nd_normalization = (post_denoising - clean_encoder_layer.batch_mean) / clean_encoder_layer.batch_std\n",
    "        self.post_denoising = post_denoising\n",
    "        self.post_2nd_normalization = post_2nd_normalization\n",
    "\n",
    "    def _denoise(self, from_left, from_above):\n",
    "        with tf.name_scope('mu') as scope:\n",
    "            mu = self._modulate(from_above)\n",
    "        with tf.name_scope('v') as scope:\n",
    "            v = self._modulate(from_above)\n",
    "        return (from_left - mu) * v + mu\n",
    "\n",
    "    def _modulate(self, u):\n",
    "        a = [_weight_variable([_layer_size(u)], name = str(i)) for i in xrange(5)]\n",
    "        return a[0] * tf.nn.sigmoid(a[1] * u + a[2]) + a[3] * u + a[4]\n",
    "\n",
    "def _weight_variable(shape, name = 'weight'):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial, name = name)\n",
    "\n",
    "def _layer_size(layer_output):\n",
    "    return layer_output.get_shape()[-1].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.837578Z",
     "start_time": "2018-09-14T02:55:45.813302Z"
    }
   },
   "outputs": [],
   "source": [
    "class Session:\n",
    "    def __init__(self, graph):\n",
    "        self.session = tf.Session()\n",
    "        self.graph = graph\n",
    "        self.writer = tf.train.SummaryWriter(logdir = strftime(\"logs/%Y-%m-%d_%H-%M-%S\"),\n",
    "                                             graph = self.session.graph)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.session.close()\n",
    "\n",
    "    def train_supervised_batch(self, inputs, labels, step_number):\n",
    "        return self._run(self.graph.supervised_train_step,\n",
    "                         summary_action = self.graph.supervised_summaries,\n",
    "                         step_number = step_number,\n",
    "                         inputs = inputs,\n",
    "                         labels = labels,\n",
    "                         is_training_phase = True)\n",
    "\n",
    "    def train_unsupervised_batch(self, inputs, step_number):\n",
    "        return self._run(self.graph.unsupervised_train_step,\n",
    "                         summary_action = self.graph.unsupervised_summaries,\n",
    "                         step_number = step_number,\n",
    "                         inputs = inputs,\n",
    "                         is_training_phase = True)\n",
    "\n",
    "    def test(self, inputs, labels, step_number):\n",
    "        result = self._run(self.graph.accuracy_measure,\n",
    "                           summary_action = self.graph.test_summaries,\n",
    "                           step_number = step_number,\n",
    "                           inputs = inputs,\n",
    "                           labels = labels,\n",
    "                           is_training_phase = False)\n",
    "        self.writer.flush()\n",
    "        return result\n",
    "\n",
    "    def save(self):\n",
    "        return self.graph.saver.save(self.session, \"checkpoints\")\n",
    "\n",
    "    def _run(self, action, summary_action, step_number, inputs, labels = None, is_training_phase = True):\n",
    "        variable_placements = self.graph.placeholders.placements(inputs, labels, is_training_phase)\n",
    "        action_result, summary = self.session.run([action, summary_action], variable_placements)\n",
    "        self.writer.add_summary(summary, step_number)\n",
    "        return action_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.867200Z",
     "start_time": "2018-09-14T02:55:45.839068Z"
    }
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self,\n",
    "        learning_rate,\n",
    "        noise_level,\n",
    "        input_layer_size,\n",
    "        class_count,\n",
    "        encoder_layer_definitions,\n",
    "        denoising_cost_multipliers):\n",
    "        assert class_count == encoder_layer_definitions[-1][0]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.denoising_cost_multipliers = denoising_cost_multipliers\n",
    "        self.placeholders = _Placeholders(input_layer_size, class_count)\n",
    "        self.output = _ForwardPass(self.placeholders,\n",
    "                                   noise_level=noise_level,\n",
    "                                   encoder_layer_definitions=encoder_layer_definitions)\n",
    "        self.accuracy_measure = self._accuracy_measure(\n",
    "            self.placeholders, self.output)\n",
    "        self.supervised_train_step = self._supervised_train_step(\n",
    "            self.placeholders, self.output)\n",
    "        self.unsupervised_train_step = self._unsupervised_train_step(\n",
    "            self.placeholders, self.output)\n",
    "\n",
    "        self.unsupervised_summaries = tf.merge_all_summaries(\"unsupervised\")\n",
    "        self.supervised_summaries = tf.merge_all_summaries(\"supervised\")\n",
    "        self.test_summaries = tf.merge_all_summaries(\"test\")\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def _accuracy_measure(self, placeholders, output):\n",
    "        with tf.name_scope(\"accuracy_measure\") as scope:\n",
    "            actual_labels = tf.argmax(output.clean_label_probabilities, 1)\n",
    "            expected_labels = tf.argmax(placeholders.labels, 1)\n",
    "            correct_prediction = tf.equal(actual_labels, expected_labels)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            tf.histogram_summary(\"class distribution\", actual_labels, [\"test\"])\n",
    "            tf.scalar_summary(\"test accuracy\", accuracy, [\"test\"])\n",
    "            return accuracy\n",
    "\n",
    "    def _supervised_train_step(self, placeholders, output):\n",
    "        with tf.name_scope(\"supervised_training\") as scope:\n",
    "            total_cost = self._total_cost(placeholders, output)\n",
    "            return self._optimizer(self.learning_rate, total_cost, [\"supervised\"])\n",
    "\n",
    "    def _unsupervised_train_step(self, placeholders, output):\n",
    "        with tf.name_scope(\"unsupervised_training\") as scope:\n",
    "            summary_tags = [\"unsupervised\"]\n",
    "            total_denoising_cost, layer_denoising_costs = self._total_denoising_cost(placeholders, output)\n",
    "            tf.scalar_summary(\"total denoising cost\", total_denoising_cost, summary_tags)\n",
    "            for index, layer_cost in enumerate(layer_denoising_costs):\n",
    "                tf.scalar_summary(\"layer %i denoising cost\" % index, layer_cost, summary_tags)\n",
    "            return self._optimizer(self.learning_rate, total_denoising_cost, summary_tags)\n",
    "\n",
    "    def _optimizer(self, learning_rate, cost_function, summary_tags):\n",
    "        with tf.name_scope(\"optimizer\") as scope:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients_and_vars = optimizer.compute_gradients(cost_function)\n",
    "            for (gradient, var) in gradients_and_vars:\n",
    "                if gradient is not None:\n",
    "                    tf.histogram_summary(\"gradient for %s\" % var.name, gradient, summary_tags)\n",
    "            return optimizer.apply_gradients(gradients_and_vars)\n",
    "\n",
    "    def _total_cost(self, placeholders, output):\n",
    "        with tf.name_scope(\"total_cost\") as scope:\n",
    "            cross_entropy = self._cross_entropy(placeholders, output)\n",
    "            total_denoising_cost, layer_denoising_costs = self._total_denoising_cost(placeholders, output)\n",
    "            total_cost = cross_entropy + total_denoising_cost\n",
    "\n",
    "            self._log_all_costs(total_cost, cross_entropy, total_denoising_cost, layer_denoising_costs, [\"supervised\"])\n",
    "\n",
    "        return total_cost\n",
    "\n",
    "    def _log_all_costs(self,\n",
    "                       total_cost = None, cross_entropy = None,\n",
    "                       total_denoising_cost = None, layer_denoising_costs = None,\n",
    "                       summary_tags = tf.GraphKeys.SUMMARIES):\n",
    "        tf.scalar_summary(\"total cost\", total_cost, summary_tags)\n",
    "\n",
    "        tf.scalar_summary(\"cross entropy\", cross_entropy, summary_tags)\n",
    "        tf.scalar_summary(\"cross entropy %\", 100 * cross_entropy / total_cost, summary_tags)\n",
    "\n",
    "        tf.scalar_summary(\"total denoising cost\", total_denoising_cost, summary_tags)\n",
    "        tf.scalar_summary(\"total denoising cost %\", 100 * total_denoising_cost / total_cost, summary_tags)\n",
    "\n",
    "        for index, layer_cost in enumerate(layer_denoising_costs):\n",
    "            tf.scalar_summary(\"layer %i denoising cost\" % index, layer_cost, summary_tags)\n",
    "            tf.scalar_summary(\"layer %i denoising cost %%\" % index, 100 * layer_cost / total_cost, summary_tags)\n",
    "\n",
    "\n",
    "    def _cross_entropy(self, placeholders, output):\n",
    "        with tf.name_scope(\"cross_entropy_cost\") as scope:\n",
    "            cross_entropy = -tf.reduce_mean(placeholders.labels * tf.log(output.corrupted_label_probabilities))\n",
    "        return cross_entropy\n",
    "\n",
    "    def _total_denoising_cost(self, placeholders, output):\n",
    "        with tf.name_scope(\"denoising_cost\") as scope:\n",
    "            layer_costs = [self._layer_denoising_cost(*params)\n",
    "                           for params in zip(output.clean_encoder_outputs,\n",
    "                                             reversed(output.decoder_outputs),\n",
    "                                             self.denoising_cost_multipliers)]\n",
    "            total_denoising_cost = sum(layer_costs)\n",
    "            return total_denoising_cost, layer_costs\n",
    "\n",
    "    def _layer_denoising_cost(self, encoder, decoder, cost_multiplier):\n",
    "        return cost_multiplier * self._mean_squared_error(encoder.pre_activation, decoder.post_2nd_normalization)\n",
    "\n",
    "    def _mean_squared_error(self, expected, actual):\n",
    "        return tf.reduce_mean(tf.pow(expected - actual, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class _Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.885380Z",
     "start_time": "2018-09-14T02:55:45.868914Z"
    }
   },
   "outputs": [],
   "source": [
    "class _Placeholders:\n",
    "    def __init__(self, input_layer_size, class_count):\n",
    "        with tf.name_scope(\"placeholders\") as scope:\n",
    "            self.inputs = tf.placeholder(tf.float32, [None, input_layer_size], name = 'inputs')\n",
    "            self.labels = tf.placeholder(tf.float32, [None, class_count], name = 'labels')\n",
    "            self.is_training_phase = tf.placeholder(tf.bool, name = 'is_training_phase')\n",
    "\n",
    "    def placements(self, inputs, labels = None, is_training_phase = True):\n",
    "        if labels is None:\n",
    "            labels = numpy.zeros([inputs.shape[0], _layer_size(self.labels)])\n",
    "        return {\n",
    "            self.inputs: inputs,\n",
    "            self.labels: labels,\n",
    "            self.is_training_phase: is_training_phase\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class _ForwardPass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.910701Z",
     "start_time": "2018-09-14T02:55:45.886913Z"
    }
   },
   "outputs": [],
   "source": [
    "class _ForwardPass:\n",
    "    def __init__(self, placeholders, encoder_layer_definitions, noise_level):\n",
    "        with tf.name_scope(\"clean_encoder\") as scope:\n",
    "            clean_encoder_outputs = self._encoder_layers(input_layer = placeholders.inputs,\n",
    "                                                         other_layer_definitions = encoder_layer_definitions,\n",
    "                                                         is_training_phase = placeholders.is_training_phase)\n",
    "\n",
    "        with tf.name_scope(\"corrupted_encoder\") as scope:\n",
    "            corrupted_encoder_outputs = self._encoder_layers(input_layer = placeholders.inputs,\n",
    "                                                             other_layer_definitions = encoder_layer_definitions,\n",
    "                                                             is_training_phase = placeholders.is_training_phase,\n",
    "                                                             noise_level = noise_level,\n",
    "                                                             reuse_variables = clean_encoder_outputs[1:])\n",
    "\n",
    "        with tf.name_scope(\"decoder\") as scope:\n",
    "            decoder_outputs = self._decoder_layers(clean_encoder_layers = clean_encoder_outputs,\n",
    "                                                   corrupted_encoder_layers = corrupted_encoder_outputs,\n",
    "                                                   is_training_phase = placeholders.is_training_phase)\n",
    "\n",
    "        self.clean_label_probabilities = clean_encoder_outputs[-1].post_activation\n",
    "        self.corrupted_label_probabilities = corrupted_encoder_outputs[-1].post_activation\n",
    "        self.autoencoded_inputs = decoder_outputs[-1]\n",
    "        self.clean_encoder_outputs = clean_encoder_outputs\n",
    "        self.corrupted_encoder_outputs = corrupted_encoder_outputs\n",
    "        self.decoder_outputs = decoder_outputs\n",
    "\n",
    "    def _encoder_layers(self,\n",
    "                        input_layer, other_layer_definitions,\n",
    "                        noise_level = None, is_training_phase = True, reuse_variables = None):\n",
    "        first_encoder_layer = _InputLayerWrapper(input_layer)\n",
    "        if reuse_variables is None:\n",
    "            reuse_variables = [None for layer in other_layer_definitions]\n",
    "        layer_accumulator = [first_encoder_layer]\n",
    "        for ((layer_size, non_linearity), reuse_layer) in zip(other_layer_definitions, reuse_variables):\n",
    "            layer_output = _EncoderLayer(inputs = layer_accumulator[-1].post_activation,\n",
    "                                         output_size = layer_size,\n",
    "                                         non_linearity = non_linearity,\n",
    "                                         noise_level = noise_level,\n",
    "                                         is_training_phase = is_training_phase,\n",
    "                                         reuse_variables = reuse_layer)\n",
    "            layer_accumulator.append(layer_output)\n",
    "        return layer_accumulator\n",
    "\n",
    "    def _decoder_layers(self, clean_encoder_layers, corrupted_encoder_layers,is_training_phase):\n",
    "        # FIXME: Actually the first decoder layer should get the correct label from above\n",
    "        encoder_layers = reversed(zip(clean_encoder_layers, corrupted_encoder_layers))\n",
    "        layer_accumulator = [None]\n",
    "        for clean_layer, corrupted_layer in encoder_layers:\n",
    "            layer = _DecoderLayer(clean_encoder_layer = clean_layer,\n",
    "                                  corrupted_encoder_layer = corrupted_layer,\n",
    "                                  previous_decoder_layer = layer_accumulator[-1],\n",
    "                                  is_training_phase = is_training_phase)\n",
    "            layer_accumulator.append(layer)\n",
    "        return layer_accumulator[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class _InputLayerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.926395Z",
     "start_time": "2018-09-14T02:55:45.912421Z"
    }
   },
   "outputs": [],
   "source": [
    "class _InputLayerWrapper:\n",
    "    def __init__(self, input_layer):\n",
    "        self.pre_activation = input_layer\n",
    "        self.post_activation = input_layer\n",
    "        self.batch_mean = tf.zeros_like(input_layer)\n",
    "        self.batch_std = tf.ones_like(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class _EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.944809Z",
     "start_time": "2018-09-14T02:55:45.927939Z"
    }
   },
   "outputs": [],
   "source": [
    "class _EncoderLayer:\n",
    "    def __init__(self, inputs, output_size, non_linearity,\n",
    "                 noise_level, is_training_phase, reuse_variables = None):\n",
    "        with tf.name_scope(\"encoder_layer\") as scope:\n",
    "            self._create_or_reuse_variables(reuse_variables, _layer_size(inputs), output_size)\n",
    "            self.pre_normalization = tf.matmul(inputs, self.weights)\n",
    "            pre_noise, self.batch_mean, self.batch_std = batch_norm(self.pre_normalization,\n",
    "                                                                    is_training_phase = is_training_phase)\n",
    "            self.pre_activation = self._add_noise(pre_noise, noise_level)\n",
    "            beta_gamma = self.gamma * (self.pre_activation + self.beta)\n",
    "            self.post_activation = non_linearity(beta_gamma)\n",
    "\n",
    "    def _create_or_reuse_variables(self, variables, input_size, output_size):\n",
    "        if variables is None:\n",
    "            self.weights = _weight_variable([input_size, output_size], name = 'W')\n",
    "            self.beta = tf.Variable(tf.constant(0.0, shape = [output_size]), name = 'beta')\n",
    "            self.gamma = tf.Variable(tf.constant(1.0, shape = [output_size]), name = 'gamma')\n",
    "        else:\n",
    "            self.weights = variables.weights\n",
    "            self.beta = variables.beta\n",
    "            self.gamma = variables.gamma\n",
    "\n",
    "    def _add_noise(self, tensor, noise_level):\n",
    "        if noise_level is None:\n",
    "            return tensor\n",
    "        else:\n",
    "            return tensor + tf.random_normal([_layer_size(tensor)], mean = 0.0, stddev = noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class _DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.963719Z",
     "start_time": "2018-09-14T02:55:45.945989Z"
    }
   },
   "outputs": [],
   "source": [
    "class _DecoderLayer:\n",
    "    def __init__(self, clean_encoder_layer, corrupted_encoder_layer,\n",
    "                 previous_decoder_layer = None, is_training_phase = True):\n",
    "        with tf.name_scope(\"decoder_layer\") as scope:\n",
    "            is_first_decoder_layer = previous_decoder_layer is None\n",
    "            if is_first_decoder_layer:\n",
    "                pre_1st_normalization = corrupted_encoder_layer.post_activation\n",
    "            else:\n",
    "                input_size = _layer_size(previous_decoder_layer.post_denoising)\n",
    "                output_size = _layer_size(clean_encoder_layer.post_activation)\n",
    "                weights = _weight_variable([input_size, output_size], name = 'V')\n",
    "                pre_1st_normalization = tf.matmul(previous_decoder_layer.post_denoising, weights)\n",
    "\n",
    "        pre_denoising, _, _ = batch_norm(pre_1st_normalization, is_training_phase = is_training_phase)\n",
    "        post_denoising = self._denoise(corrupted_encoder_layer.pre_activation, pre_denoising)\n",
    "        post_2nd_normalization = (post_denoising - clean_encoder_layer.batch_mean) / clean_encoder_layer.batch_std\n",
    "        self.post_denoising = post_denoising\n",
    "        self.post_2nd_normalization = post_2nd_normalization\n",
    "\n",
    "    def _denoise(self, from_left, from_above):\n",
    "        with tf.name_scope('mu') as scope:\n",
    "            mu = self._modulate(from_above)\n",
    "        with tf.name_scope('v') as scope:\n",
    "            v = self._modulate(from_above)\n",
    "        return (from_left - mu) * v + mu\n",
    "\n",
    "    def _modulate(self, u):\n",
    "        a = [_weight_variable([_layer_size(u)], name = str(i)) for i in xrange(5)]\n",
    "        return a[0] * tf.nn.sigmoid(a[1] * u + a[2]) + a[3] * u + a[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.982576Z",
     "start_time": "2018-09-14T02:55:45.964954Z"
    }
   },
   "outputs": [],
   "source": [
    "def _weight_variable(shape, name = 'weight'):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial, name = name)\n",
    "\n",
    "def _layer_size(layer_output):\n",
    "    return layer_output.get_shape()[-1].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:45.998463Z",
     "start_time": "2018-09-14T02:55:45.984184Z"
    }
   },
   "outputs": [],
   "source": [
    "import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ladder_on_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:46.388425Z",
     "start_time": "2018-09-14T02:55:45.999706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "50000 unlabeled training examples\n",
      "5000 labeled training examples\n",
      "5000 validation examples\n",
      "10000 test examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading MNIST data\")\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",\n",
    "                                  one_hot=True,\n",
    "                                  labeled_size=5000,\n",
    "                                  validation_size=5000)\n",
    "\n",
    "print(mnist.train_unlabeled.num_examples, \"unlabeled training examples\")\n",
    "print(mnist.train_labeled.num_examples, \"labeled training examples\")\n",
    "print(mnist.validation.num_examples, \"validation examples\")\n",
    "print(mnist.test.num_examples, \"test examples\")\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"noise_level\": 0.2,\n",
    "    \"input_layer_size\": 784,\n",
    "    \"class_count\": 10,\n",
    "    \"encoder_layer_definitions\": [\n",
    "        (100, tf.nn.relu), # first hidden layer\n",
    "        (50, tf.nn.relu),\n",
    "        (10, tf.nn.softmax) # output layer\n",
    "    ],\n",
    "    \"denoising_cost_multipliers\": [\n",
    "        1000, # input layer\n",
    "        0.5, # first hidden layer\n",
    "        0.1,\n",
    "        0.1 # output layer\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:46.731583Z",
     "start_time": "2018-09-14T02:55:46.389865Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument to reversed() must be a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-faa1bc50334b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-62b89484f1cc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, learning_rate, noise_level, input_layer_size, class_count, encoder_layer_definitions, denoising_cost_multipliers)\u001b[0m\n\u001b[1;32m     13\u001b[0m         self.output = _ForwardPass(self.placeholders,\n\u001b[1;32m     14\u001b[0m                                    \u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                    encoder_layer_definitions=encoder_layer_definitions)\n\u001b[0m\u001b[1;32m     16\u001b[0m         self.accuracy_measure = self._accuracy_measure(\n\u001b[1;32m     17\u001b[0m             self.placeholders, self.output)\n",
      "\u001b[0;32m<ipython-input-11-76eced148c5d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, placeholders, encoder_layer_definitions, noise_level)\u001b[0m\n\u001b[1;32m     16\u001b[0m             decoder_outputs = self._decoder_layers(clean_encoder_layers = clean_encoder_outputs,\n\u001b[1;32m     17\u001b[0m                                                    \u001b[0mcorrupted_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrupted_encoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                    is_training_phase = placeholders.is_training_phase)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_label_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_encoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-76eced148c5d>\u001b[0m in \u001b[0;36m_decoder_layers\u001b[0;34m(self, clean_encoder_layers, corrupted_encoder_layers, is_training_phase)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decoder_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_training_phase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# FIXME: Actually the first decoder layer should get the correct label from above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mencoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_encoder_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mlayer_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclean_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument to reversed() must be a sequence"
     ]
    }
   ],
   "source": [
    "graph = Graph(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T02:55:46.732278Z",
     "start_time": "2018-09-14T02:55:43.154Z"
    }
   },
   "outputs": [],
   "source": [
    "with ladder_network.Session(graph) as session:\n",
    "    for step in xrange(1000):\n",
    "        if step % 5 == 0:\n",
    "            images, labels = mnist.train_labeled.next_batch(100)\n",
    "            session.train_supervised_batch(images, labels, step)\n",
    "            else:\n",
    "                images, _ = mnist.train_unlabeled.next_batch(100)\n",
    "                session.train_unsupervised_batch(images, step)\n",
    "        if step % 200 == 0:\n",
    "            save_path = session.save()\n",
    "            accuracy = session.test(mnist.validation.images, mnist.validation.labels, step)\n",
    "            print()\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            print(\"Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "511px",
    "left": "40px",
    "top": "136px",
    "width": "220px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
